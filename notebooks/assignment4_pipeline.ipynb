{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 4 Ransomware Classification Pipeline\n",
        "\n",
        "This notebook reproduces the end-to-end workflow delivered in the Assignment 4 coding tasks. Run the cells sequentially to fine-tune the transformer models, evaluate them on the UGR and PM datasets, and generate the diagnostic artefacts (metrics, plots, SHAP/LIME reports)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment preparation\n",
        "\n",
        "Uncomment the installation command below if you are running in a fresh environment that does not already have the required dependencies. The command expects to be executed from the repository root so that the relative path to `requirements.txt` resolves correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure paths and imports\n",
        "\n",
        "The cell below detects the project root, adds the `src` directory to `sys.path`, and imports the helpers that power the assignment pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import random\n",
        "from dataclasses import replace\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "if not (PROJECT_ROOT / 'src').exists():\n",
        "    # When the notebook is opened from the `notebooks/` directory, move one level up.\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "SRC_DIR = PROJECT_ROOT / 'src'\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "print(f'Project root: {PROJECT_ROOT}')\n",
        "print(f'Source directory added to sys.path: {SRC_DIR}')\n",
        "\n",
        "from assignment4.config import AssignmentConfig, DEFAULT_MODELS\n",
        "from assignment4.data_utils import (\n",
        "    build_datasets,\n",
        "    load_dataframe,\n",
        "    save_split_metadata,\n",
        "    stratified_split,\n",
        ")\n",
        "from assignment4.explainability import generate_lime_reports, generate_shap_plots\n",
        "from assignment4.modeling import load_model_components\n",
        "from assignment4.training import train_model\n",
        "from assignment4.visualisation import plot_attention_heatmap, plot_training_history\n",
        "\n",
        "print('Imports complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Helper utilities\n",
        "\n",
        "Some small helpers mirror the orchestration script so the notebook can orchestrate the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_models(model_names, num_epochs=None):\n",
        "    \"\"\"Pick the configured transformer models and optionally override epochs.\"\"\"\n",
        "    selected = []\n",
        "    for cfg in DEFAULT_MODELS:\n",
        "        if cfg.name in {name.lower() for name in model_names}:\n",
        "            selected_cfg = cfg\n",
        "            if num_epochs is not None and cfg.num_train_epochs != num_epochs:\n",
        "                selected_cfg = replace(cfg, num_train_epochs=num_epochs)\n",
        "            selected.append(selected_cfg)\n",
        "    if not selected:\n",
        "        raise ValueError(f'No matching models found for {model_names!r}')\n",
        "    return selected\n",
        "\n",
        "\n",
        "def sample_texts(texts, k, seed):\n",
        "    \"\"\"Return up to *k* texts sampled deterministically using the provided seed.\"\"\"\n",
        "    if k is None or k <= 0:\n",
        "        return list(texts)\n",
        "    rng = random.Random(seed)\n",
        "    texts = list(texts)\n",
        "    if len(texts) <= k:\n",
        "        return texts\n",
        "    return rng.sample(texts, k)\n",
        "\n",
        "\n",
        "def display_summary(metrics):\n",
        "    \"\"\"Render a pandas DataFrame with the collected evaluation metrics.\"\"\"\n",
        "    if not metrics:\n",
        "        return pd.DataFrame()\n",
        "    df = pd.DataFrame(metrics).T\n",
        "    df.index.name = 'dataset_model'\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure the experiment\n",
        "\n",
        "Adjust the parameters in the next cell to control which models run, how many samples are used, and whether explainability artefacts are produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASETS = {\n",
        "    'ugr': PROJECT_ROOT / 'UGR_text.csv',\n",
        "    'pm': PROJECT_ROOT / 'PM_text.csv',\n",
        "}\n",
        "\n",
        "OUTPUT_DIR = PROJECT_ROOT / 'notebook_artifacts'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "MAX_SAMPLES = None  # e.g. set to 500 for a quicker dry run\n",
        "SELECTED_MODELS = ['bert', 'roberta', 'deberta']\n",
        "EPOCH_OVERRIDE = 1  # reduce epochs to keep runtime manageable in a notebook\n",
        "GENERATE_ATTENTION = True\n",
        "GENERATE_EXPLAINABILITY = True  # toggle off if SHAP/LIME should be skipped\n",
        "\n",
        "assignment_cfg = AssignmentConfig(random_seed=RANDOM_SEED, output_dir=str(OUTPUT_DIR))\n",
        "assignment_cfg.shap_sample_size = min(assignment_cfg.shap_sample_size, 10)\n",
        "assignment_cfg.lime_sample_size = min(assignment_cfg.lime_sample_size, 5)\n",
        "assignment_cfg.attention_plot_examples = min(assignment_cfg.attention_plot_examples, 3)\n",
        "assignment_cfg.model_cache_dir = str(PROJECT_ROOT / 'model_cache')\n",
        "\n",
        "model_configs = select_models(SELECTED_MODELS, num_epochs=EPOCH_OVERRIDE)\n",
        "print('Configured models:', [cfg.name for cfg in model_configs])\n",
        "print('Artifacts will be written to:', OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run the training and evaluation loop\n",
        "\n",
        "Executing the next cell performs the complete workflow for each dataset/model combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_metrics = {}\n",
        "\n",
        "for dataset_name, csv_path in DATASETS.items():\n",
        "    print(f'\\n=== Processing dataset: {dataset_name.upper()} ===')\n",
        "    df = load_dataframe(csv_path)\n",
        "    if MAX_SAMPLES is not None:\n",
        "        df = df.sample(n=min(MAX_SAMPLES, len(df)), random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "    train_df, val_df, test_df = stratified_split(df, assignment_cfg)\n",
        "    save_split_metadata(OUTPUT_DIR, dataset_name, train_df, val_df, test_df, assignment_cfg)\n",
        "\n",
        "    for model_cfg in model_configs:\n",
        "        print(f'\nTraining model: {model_cfg.name}')\n",
        "        components = load_model_components(\n",
        "            model_cfg,\n",
        "            num_labels=len(assignment_cfg.label_names),\n",
        "            cache_dir=assignment_cfg.model_cache_dir,\n",
        "        )\n",
        "        train_ds, val_ds, test_ds = build_datasets(\n",
        "            components.tokenizer,\n",
        "            train_df,\n",
        "            val_df,\n",
        "            test_df,\n",
        "            model_cfg,\n",
        "        )\n",
        "        training_result = train_model(\n",
        "            components,\n",
        "            train_ds,\n",
        "            val_ds,\n",
        "            test_ds,\n",
        "            dataset_name,\n",
        "            model_cfg,\n",
        "            assignment_cfg,\n",
        "            OUTPUT_DIR,\n",
        "        )\n",
        "        summary_key = f'{dataset_name}_{model_cfg.name}'\n",
        "        summary_metrics[summary_key] = training_result.eval_metrics\n",
        "        plot_training_history(training_result.train_history, OUTPUT_DIR, dataset_name, model_cfg.name)\n",
        "\n",
        "        if GENERATE_ATTENTION:\n",
        "            attention_samples = sample_texts(test_df['text'].tolist(), assignment_cfg.attention_plot_examples, RANDOM_SEED)\n",
        "            plot_attention_heatmap(\n",
        "                training_result.trainer.model,\n",
        "                components.tokenizer,\n",
        "                attention_samples,\n",
        "                OUTPUT_DIR,\n",
        "                dataset_name,\n",
        "                model_cfg,\n",
        "                max_examples=assignment_cfg.attention_plot_examples,\n",
        "            )\n",
        "\n",
        "        if GENERATE_EXPLAINABILITY:\n",
        "            shap_texts = sample_texts(test_df['text'].tolist(), assignment_cfg.shap_sample_size, RANDOM_SEED)\n",
        "            generate_shap_plots(\n",
        "                training_result.trainer.model,\n",
        "                components.tokenizer,\n",
        "                shap_texts,\n",
        "                OUTPUT_DIR,\n",
        "                dataset_name,\n",
        "                model_cfg,\n",
        "                [assignment_cfg.label_names[i] for i in sorted(assignment_cfg.label_names)],\n",
        "            )\n",
        "\n",
        "            lime_texts = sample_texts(test_df['text'].tolist(), assignment_cfg.lime_sample_size, RANDOM_SEED)\n",
        "            generate_lime_reports(\n",
        "                training_result.trainer.model,\n",
        "                components.tokenizer,\n",
        "                lime_texts,\n",
        "                OUTPUT_DIR,\n",
        "                dataset_name,\n",
        "                model_cfg,\n",
        "                [assignment_cfg.label_names[i] for i in sorted(assignment_cfg.label_names)],\n",
        "            )\n",
        "\n",
        "summary_path = OUTPUT_DIR / 'summary_metrics.json'\n",
        "summary_path.write_text(json.dumps(summary_metrics, indent=2))\n",
        "print('\nSaved summary metrics to', summary_path)\n",
        "summary_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inspect evaluation metrics\n",
        "\n",
        "Convert the aggregated metrics into a table for convenient inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = display_summary(summary_metrics)\n",
        "metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Explore generated artefacts\n",
        "\n",
        "All artefacts (metrics, plots, SHAP/LIME outputs) are written to the directory configured above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted(path.name for path in OUTPUT_DIR.glob('*'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}